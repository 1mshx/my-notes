Urllib

爬虫：使用程序模拟浏览器，去向服务器发送请求，获取响应信息

爬虫分类：
(1)通用爬虫：功能：访问网页 -> 抓取数据 -> 数据存储 -> 数据处理 -> 提供检索服务
	robots协议：一个约定俗称的协议，添加robots.txt文件，说明本网站哪些内容不可以被抓取，起不到限制作用，自己写的爬虫无需遵守
	缺点：1.抓取的数据大多都是无用的
	2.不能根据用户的需求来精确获取数据
(2)聚焦爬虫：功能 根据需求，实现爬虫程序，抓取需要的数据
	设计思路：1.确定要爬取的url
	2.模拟浏览器通过http协议访问url，获取服务器返回的html代码
	3.解析html字符串

反爬手段：
1.User-Agent：用户代理，是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本，CPU类型，浏览器及其版本，浏览器渲染引擎，浏览器语言，浏览器插件等。
2.代理IP：西次代理，快代理
3.验证码访问：打码平台，云打码平台
4.动态加载网页：网站返回的是js数据，并不是网页的真实数据
5.数据加密：分析js代码

基本使用：
	1.导包
	import urllib.request
	2.定义想要访问的网址
	url='http://baidu.com'
	3.模拟浏览器向服务器发送请求
	response=urllib.request.urlopen(url)
	4.获取响应内容
	content=response.read().decode('utf-8')

一个类型六个方法：
resopnse是HTTPResponse类型
resopnse.read().decode('utf-8')：按照一个字节一个字节的去读
resopnse.read(5).decode('utf-8')：里面数字多少返回多少
resopnse.readline().decode('utf-8')：按照一行一行的去读
resopnse.readlines().decode('utf-8')：按行读取，读取所有
resopnse.getcode()：返回状态码，如果是200，说明没有问题
resopnse.geturl()：返回url地址

请求对象的定制：
# url的组成
# https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&tn=baidu&wd=%E5%91%A8%E6%9D%B0%E4%BC%A6
# http/https   www.baidu.com   80/443    s    ?后面的    #
#    协议           主机        端口号    路径  参数     锚点
# http      80
# https     443
# mysql     3306
# oracle    1521
# redis     6379
# mongodb   27017

	1.定义一个请求头
	headers = {
	    'User_Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 '
	                  'Safari/537.36 '
	}
	2.合并成请求对象
	request = urllib.request.Request(url=url, headers=headers)
	3.模拟向服务器发送请求
	response = urllib.request.urlopen(request)

下载：
网页：urllib.request.urlretrieve(视频地址,'存储地址.html')
图片：urllib.request.urlretrieve(图片地址,'存储地址.jpg')
视频：urllib.request.urlretrieve(视频地址,'存储地址.mp4')

编码：
urllib.parse.quto(内容)：单个内容进行编码
urllib.parse.urlencode(字典)：对多个内容参数进行编码

post请求：
	1.post请求的参数，必须进行编码
	data = urllib.parse.urlencode(data).encode('utf-8')
	2.转换为json数据
	obj = json.loads(content)

爬虫中的异常：URLError和HTTPError
request.HTTPError是URLError的子类，表示在发起http请求时出现错误
request.URLError错误一般是主机地址和参数出了问题
request.URLRequired格式出错，格式问题
request.ConnectionError由于DNS查询失败的异常

cookie登录：

request请求头：
	headers = {
	    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 '
	                  'Safari/537.36',
	    ':authority': 'weibo.com',
	    ':method': 'GET',
	    ':path': '/ajax/profile/info?uid=7848081846',
	    ':scheme': 'https',
	    'Accept': 'application/json, text/plain, */*',
	    'Accept-Encoding': 'gzip, deflate, br',
	    'Accept-Language': 'zh-CN,zh;q=0.9',
	    'Client-Version': 'v2.43.4',
	    'Cookie': ,   # Cookie携带着你的登录信息，如果有登录之后的cookie，那么我们就可以携带着cookie进入任何页面
	    'Referer': ,   # 防盗链，判断当前路径是不是由上一个路径进来的，如果info链接不是通过这个进来的，那么会报错，一般用来做图片防盗链
	    'Sec-Ch-Ua': '"Not/A)Brand";v="99", "Google Chrome";v="115", "Chromium";v="115"',
	    'Sec-Ch-Ua-Mobile': '?0',
	    'Sec-Ch-Ua-Platform': 'Windows',
	    'Sec-Fetch-Dest': 'empty',
	    'Sec-Fetch-Mode': 'cors',
	    'Sec-Fetch-Site': 'same-origin',
	    'Server-Version': 'v2023.07.27.1',
	    'X-Requested-With': 'XMLHttpRequest',
	    'X-Xsrf-Token': 'KntlQj_Y_oInkVZSIEGQjDDO',
	}
	1.带：的和Accept-Encoding的全都不好使，直接注释
	2.Cookie携带着你的登录信息，如果有登录之后的cookie，那么我们就可以携带着cookie进入任何页面
	3.Referer防盗链，判断当前路径是不是由上一个路径进来的，如果info链接不是通过这个进来的，那么会报错，一般用来做图片防盗链

Handler处理器：
	1.url.request.urlopen(url)
		不能定制请求头
	2.url.request.Reurest(url,headers,data)
		可以定制请求头
	3.Handler
		定制更高级的请求头(随着业务逻辑的复杂，请求对象的定制已经不能满足我们的需求，比如动态cookie和代理不能使用请求对象的定制)
三步：
	(1)获取handler对象
		handler = urllib.request.HTTPHandler()
	(2)获取opener对象
		opener = urllib.request.build_opener(handler)
	(3)调用open方法
		response = opener.open(request)

代理服务器：
	代理的常用功能：
		1.突破自身IP的访问限制，访问国外站点
		2.访问一些单位或团体的内部资源
		3.提高访问速度
		4.隐藏真实IP

	代码配置代理：
		1.创建Request对象
		2.创建ProxyHandler对象
		3.用handler对象创建opener对象
		eg:	proxies = {
			    'http': '36.255.85.218:32650'
			}
			handler = urllib.request.ProxyHandler(proxies=proxies)
			opener = urllib.request.build_opener(handler)
			response = opener.open(request)
